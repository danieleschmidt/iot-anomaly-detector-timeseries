name: Performance Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday at 6 AM

jobs:
  performance-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-profiling.txt
        pip install pytest-benchmark memory-profiler psutil
    
    - name: Run performance benchmarks
      run: |
        python -m pytest benchmarks/ -v --benchmark-json=benchmark-results.json
        python -m pytest tests/performance/ -v
    
    - name: Memory profiling
      run: |
        python -m memory_profiler src/train_autoencoder.py --data data/raw/sensor_data.csv --epochs 1
        python -c "
        import psutil
        import json
        import os
        
        # System memory info
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            'memory_total_gb': round(memory.total / (1024**3), 2),
            'memory_available_gb': round(memory.available / (1024**3), 2),
            'memory_percent': memory.percent,
            'disk_total_gb': round(disk.total / (1024**3), 2),
            'disk_free_gb': round(disk.free / (1024**3), 2),
            'cpu_count': psutil.cpu_count()
        }
        
        with open('system-metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        "
    
    - name: Model performance test
      run: |
        python validate_performance_monitoring.py --quick-test
        python validate_memory_efficient.py --quick-test
    
    - name: Load testing (if API exists)
      run: |
        if [ -f "src/model_serving_api.py" ]; then
          echo "Running API load tests"
          python -c "
          import subprocess
          import time
          import requests
          import threading
          
          # Start API in background
          proc = subprocess.Popen(['python', 'src/model_serving_api.py'], 
                                stdout=subprocess.PIPE, 
                                stderr=subprocess.PIPE)
          time.sleep(5)  # Wait for startup
          
          try:
              response = requests.get('http://localhost:8000/health', timeout=5)
              print(f'API Health Check: {response.status_code}')
          except:
              print('API not available for load testing')
          finally:
              proc.terminate()
          "
        fi
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: |
          benchmark-results.json
          system-metrics.json
          *.prof
    
    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let comment = '## üöÄ Performance Test Results\\n\\n';
          
          try {
            const benchmarks = JSON.parse(fs.readFileSync('benchmark-results.json', 'utf8'));
            comment += '### Benchmark Results\\n';
            benchmarks.benchmarks.forEach(b => {
              comment += `- **${b.name}**: ${b.stats.mean.toFixed(4)}s (¬±${b.stats.stddev.toFixed(4)}s)\\n`;
            });
          } catch (e) {
            comment += '### Benchmark Results\\nNo benchmark results available\\n';
          }
          
          try {
            const metrics = JSON.parse(fs.readFileSync('system-metrics.json', 'utf8'));
            comment += '\\n### System Resources\\n';
            comment += `- **Memory Usage**: ${metrics.memory_percent}% (${metrics.memory_available_gb}GB available)\\n`;
            comment += `- **CPU Cores**: ${metrics.cpu_count}\\n`;
            comment += `- **Disk Space**: ${metrics.disk_free_gb}GB free\\n`;
          } catch (e) {
            comment += '\\n### System Resources\\nNo system metrics available\\n';
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  regression-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark
    
    - name: Run baseline benchmarks (main branch)
      run: |
        git checkout main
        python -m pytest benchmarks/ --benchmark-json=baseline-results.json
        git checkout ${{ github.head_ref }}
    
    - name: Run current benchmarks
      run: |
        python -m pytest benchmarks/ --benchmark-json=current-results.json
    
    - name: Compare performance
      run: |
        python -c "
        import json
        import sys
        
        try:
            with open('baseline-results.json') as f:
                baseline = json.load(f)
            with open('current-results.json') as f:
                current = json.load(f)
            
            print('Performance Comparison:')
            for b_bench in baseline['benchmarks']:
                for c_bench in current['benchmarks']:
                    if b_bench['name'] == c_bench['name']:
                        b_mean = b_bench['stats']['mean']
                        c_mean = c_bench['stats']['mean']
                        change = ((c_mean - b_mean) / b_mean) * 100
                        status = 'üî¥' if change > 10 else 'üü°' if change > 5 else 'üü¢'
                        print(f'{status} {b_bench[\"name\"]}: {change:+.2f}% change')
                        
                        if change > 20:  # Fail if performance degrades by more than 20%
                            print(f'‚ùå Significant performance regression detected!')
                            sys.exit(1)
        except Exception as e:
            print(f'Could not compare performance: {e}')
        "