name: Performance Testing

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main]
  schedule:
    - cron: '0 3 * * 1' # Weekly on Mondays at 3 AM
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - benchmark
          - load
          - memory
          - stress

env:
  PYTHON_VERSION: '3.12'

jobs:
  # Benchmark testing
  benchmark:
    name: Benchmark Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'benchmark' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install pytest-benchmark memory-profiler

      - name: Run benchmark tests
        run: |
          echo "=== Running Benchmark Tests ==="
          pytest tests/performance/ -v \
            --benchmark-json=benchmark-results.json \
            --benchmark-min-rounds=5 \
            --benchmark-warmup=true \
            --benchmark-sort=mean \
            -m "benchmark"

      - name: Performance regression check
        run: |
          echo "=== Performance Regression Check ==="
          
          # Create performance comparison script
          cat > compare_performance.py << 'EOF'
          import json
          import sys
          
          def compare_benchmarks(current_file, baseline_threshold=1.2):
              try:
                  with open(current_file, 'r') as f:
                      current = json.load(f)
                  
                  # Extract benchmark results
                  benchmarks = current.get('benchmarks', [])
                  
                  performance_issues = []
                  
                  for benchmark in benchmarks:
                      name = benchmark['name']
                      mean_time = benchmark['stats']['mean']
                      
                      # Define baseline expectations (in seconds)
                      baselines = {
                          'test_data_preprocessing': 0.1,
                          'test_model_training': 5.0,
                          'test_anomaly_detection': 0.5,
                          'test_batch_processing': 2.0
                      }
                      
                      baseline = baselines.get(name.split('[')[0], 1.0)
                      
                      if mean_time > baseline * baseline_threshold:
                          performance_issues.append({
                              'test': name,
                              'current': mean_time,
                              'baseline': baseline,
                              'ratio': mean_time / baseline
                          })
                  
                  if performance_issues:
                      print("âš ï¸ Performance regression detected:")
                      for issue in performance_issues:
                          print(f"  - {issue['test']}: {issue['current']:.3f}s (baseline: {issue['baseline']:.3f}s, {issue['ratio']:.1f}x slower)")
                      return False
                  else:
                      print("âœ… All benchmarks within acceptable performance range")
                      return True
                      
              except Exception as e:
                  print(f"Error comparing benchmarks: {e}")
                  return True  # Don't fail on comparison errors
          
          if __name__ == "__main__":
              success = compare_benchmarks('benchmark-results.json')
              sys.exit(0 if success else 1)
          EOF
          
          python compare_performance.py

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json

  # Load testing
  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    
    services:
      api:
        image: python:3.12
        ports:
          - 8000:8000
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install locust requests

      - name: Start API server
        run: |
          echo "=== Starting API Server ==="
          # Generate test model
          python -m src.generate_data --num-samples 1000 --num-features 3 --output-path data/raw/test_data.csv
          python -m src.train_autoencoder --epochs 2 --window-size 30 --latent-dim 16 --model-path saved_models/test_model.h5 --scaler-path saved_models/test_scaler.pkl
          
          # Start API server in background
          python -m src.model_serving_api --port 8000 &
          sleep 10
          
          # Test server is running
          curl -f http://localhost:8000/health || exit 1

      - name: Run load tests
        run: |
          echo "=== Running Load Tests ==="
          
          # Create Locust test file
          cat > locustfile.py << 'EOF'
          from locust import HttpUser, task, between
          import json
          import random
          
          class APIUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  # Health check
                  self.client.get("/health")
              
              @task(3)
              def predict_anomaly(self):
                  # Generate sample data
                  data = {
                      "data": [[random.uniform(-1, 1) for _ in range(3)] for _ in range(30)]
                  }
                  self.client.post("/predict", json=data)
              
              @task(1)
              def get_model_info(self):
                  self.client.get("/model/info")
          EOF
          
          # Run load test
          locust -f locustfile.py --host=http://localhost:8000 \
            --users=10 --spawn-rate=2 --run-time=60s --headless \
            --html=load-test-report.html --csv=load-test-results

      - name: Analyze load test results
        run: |
          echo "=== Load Test Analysis ==="
          
          if [ -f "load-test-results_stats.csv" ]; then
              echo "Load test completed successfully"
              cat load-test-results_stats.csv
              
              # Check for failures
              failure_count=$(tail -n +2 load-test-results_stats.csv | cut -d',' -f4 | awk '{sum+=$1} END {print sum}')
              if [ "$failure_count" -gt 0 ]; then
                  echo "âš ï¸ Warning: $failure_count requests failed during load test"
              else
                  echo "âœ… All requests succeeded during load test"
              fi
          else
              echo "âŒ Load test results not found"
              exit 1
          fi

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results*.csv

  # Memory profiling
  memory-profile:
    name: Memory Profiling
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'memory' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install memory-profiler psutil matplotlib

      - name: Run memory profiling
        run: |
          echo "=== Memory Profiling ==="
          
          # Create memory profiling script
          cat > memory_profile.py << 'EOF'
          import psutil
          import os
          import time
          import matplotlib.pyplot as plt
          from memory_profiler import profile
          import numpy as np
          
          def monitor_memory():
              process = psutil.Process(os.getpid())
              
              # Test data generation
              print("Testing data generation...")
              from src.generate_data import generate_sensor_data
              memory_before = process.memory_info().rss / 1024 / 1024
              
              data = generate_sensor_data(10000, 3, anomaly_start=5000, anomaly_length=100)
              memory_after = process.memory_info().rss / 1024 / 1024
              
              print(f"Data generation: {memory_before:.2f}MB -> {memory_after:.2f}MB (diff: {memory_after - memory_before:.2f}MB)")
              
              # Test preprocessing
              print("Testing data preprocessing...")
              from src.data_preprocessor import DataPreprocessor
              memory_before = process.memory_info().rss / 1024 / 1024
              
              preprocessor = DataPreprocessor()
              processed = preprocessor.fit_transform(data)
              memory_after = process.memory_info().rss / 1024 / 1024
              
              print(f"Data preprocessing: {memory_before:.2f}MB -> {memory_after:.2f}MB (diff: {memory_after - memory_before:.2f}MB)")
              
              # Memory usage summary
              peak_memory = process.memory_info().rss / 1024 / 1024
              print(f"Peak memory usage: {peak_memory:.2f}MB")
              
              if peak_memory > 500:  # 500MB threshold
                  print("âš ï¸ Warning: High memory usage detected")
                  return False
              else:
                  print("âœ… Memory usage within acceptable limits")
                  return True
          
          if __name__ == "__main__":
              success = monitor_memory()
              exit(0 if success else 1)
          EOF
          
          python memory_profile.py

      - name: Generate memory report
        run: |
          echo "=== Generating Memory Report ==="
          
          # Create detailed memory analysis
          cat > detailed_memory_analysis.py << 'EOF'
          import psutil
          import gc
          import sys
          
          def analyze_memory():
              # System memory info
              memory = psutil.virtual_memory()
              print(f"System Memory:")
              print(f"  Total: {memory.total / 1024 / 1024 / 1024:.2f} GB")
              print(f"  Available: {memory.available / 1024 / 1024 / 1024:.2f} GB")
              print(f"  Used: {memory.percent}%")
              
              # Process memory info
              process = psutil.Process()
              memory_info = process.memory_info()
              print(f"\nProcess Memory:")
              print(f"  RSS: {memory_info.rss / 1024 / 1024:.2f} MB")
              print(f"  VMS: {memory_info.vms / 1024 / 1024:.2f} MB")
              
              # Python memory info
              print(f"\nPython Memory:")
              print(f"  Object count: {len(gc.get_objects())}")
              print(f"  Reference count: {sys.gettotalrefcount() if hasattr(sys, 'gettotalrefcount') else 'N/A'}")
          
          if __name__ == "__main__":
              analyze_memory()
          EOF
          
          python detailed_memory_analysis.py

  # Stress testing
  stress-test:
    name: Stress Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run stress tests
        run: |
          echo "=== Running Stress Tests ==="
          
          # Create stress test script
          cat > stress_test.py << 'EOF'
          import time
          import threading
          import queue
          import numpy as np
          from concurrent.futures import ThreadPoolExecutor, as_completed
          
          def stress_test_data_processing():
              print("Stress testing data processing...")
              
              from src.data_preprocessor import DataPreprocessor
              
              def process_batch(batch_id):
                  try:
                      # Generate large dataset
                      data = np.random.randn(1000, 3)
                      preprocessor = DataPreprocessor()
                      processed = preprocessor.fit_transform(data)
                      return f"Batch {batch_id}: Success"
                  except Exception as e:
                      return f"Batch {batch_id}: Failed - {e}"
              
              # Run concurrent processing
              results = []
              with ThreadPoolExecutor(max_workers=4) as executor:
                  futures = [executor.submit(process_batch, i) for i in range(10)]
                  for future in as_completed(futures):
                      results.append(future.result())
              
              for result in results:
                  print(f"  {result}")
              
              success_count = sum(1 for r in results if "Success" in r)
              print(f"Stress test completed: {success_count}/10 batches successful")
              
              return success_count >= 8  # At least 80% success rate
          
          def stress_test_memory_allocation():
              print("Stress testing memory allocation...")
              
              try:
                  # Gradually increase memory usage
                  data_blocks = []
                  for i in range(50):  # 50 blocks of 10MB each
                      block = np.random.randn(1000000)  # ~10MB
                      data_blocks.append(block)
                      
                      if i % 10 == 0:
                          print(f"  Allocated {i+1} blocks (~{(i+1)*10}MB)")
                  
                  print("  Memory allocation test completed successfully")
                  return True
                  
              except MemoryError:
                  print("  MemoryError encountered - this is expected under stress")
                  return True
              except Exception as e:
                  print(f"  Unexpected error: {e}")
                  return False
          
          if __name__ == "__main__":
              test1_passed = stress_test_data_processing()
              test2_passed = stress_test_memory_allocation()
              
              if test1_passed and test2_passed:
                  print("âœ… All stress tests passed")
                  exit(0)
              else:
                  print("âŒ Some stress tests failed")
                  exit(1)
          EOF
          
          python stress_test.py

  # Performance summary
  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [benchmark, load-test, memory-profile, stress-test]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-results/

      - name: Generate performance report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Collect results from all jobs
            const jobs = [
              { name: 'benchmark', result: '${{ needs.benchmark.result }}' },
              { name: 'load-test', result: '${{ needs.load-test.result }}' },
              { name: 'memory-profile', result: '${{ needs.memory-profile.result }}' },
              { name: 'stress-test', result: '${{ needs.stress-test.result }}' }
            ];
            
            let reportBody = '## ðŸš€ Performance Test Report\n\n';
            reportBody += `**Workflow Run**: ${context.runId}\n`;
            reportBody += `**Commit**: ${context.sha.substring(0, 7)}\n`;
            reportBody += `**Branch**: ${context.ref.replace('refs/heads/', '')}\n\n`;
            
            reportBody += '### ðŸ“Š Test Results\n\n';
            
            const passedJobs = jobs.filter(job => job.result === 'success');
            const failedJobs = jobs.filter(job => job.result === 'failure');
            const skippedJobs = jobs.filter(job => job.result === 'skipped');
            
            jobs.forEach(job => {
              const emoji = job.result === 'success' ? 'âœ…' : 
                           job.result === 'failure' ? 'âŒ' : 
                           job.result === 'skipped' ? 'â­ï¸' : 'â“';
              reportBody += `- ${emoji} **${job.name}**: ${job.result}\n`;
            });
            
            reportBody += '\n### ðŸ“ˆ Performance Metrics\n\n';
            
            if (passedJobs.length === jobs.length) {
              reportBody += 'ðŸŽ‰ **All performance tests passed!** The system is performing within expected parameters.\n\n';
            } else if (failedJobs.length > 0) {
              reportBody += 'âš ï¸ **Performance issues detected.** Please review the failed tests and investigate potential optimizations.\n\n';
            }
            
            reportBody += '### ðŸ”— Artifacts\n\n';
            reportBody += '- Benchmark results\n';
            reportBody += '- Load test reports\n';
            reportBody += '- Memory profiling data\n';
            reportBody += '- Stress test results\n\n';
            
            reportBody += '### ðŸŽ¯ Recommendations\n\n';
            
            if (failedJobs.some(job => job.name === 'benchmark')) {
              reportBody += '- Review benchmark failures for performance regressions\n';
            }
            if (failedJobs.some(job => job.name === 'load-test')) {
              reportBody += '- Investigate load test failures for scalability issues\n';
            }
            if (failedJobs.some(job => job.name === 'memory-profile')) {
              reportBody += '- Check memory usage patterns for optimization opportunities\n';
            }
            if (failedJobs.some(job => job.name === 'stress-test')) {
              reportBody += '- Review stress test failures for stability improvements\n';
            }
            
            // Create performance tracking issue if there are failures
            if (failedJobs.length > 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'ðŸš€ Performance Test Results - Issues Detected',
                body: reportBody,
                labels: ['performance', 'testing', 'needs-investigation']
              });
            }
            
            // Always log the summary
            console.log('Performance Test Summary:');
            console.log(`- Passed: ${passedJobs.length}`);
            console.log(`- Failed: ${failedJobs.length}`);
            console.log(`- Skipped: ${skippedJobs.length}`);